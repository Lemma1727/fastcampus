{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNbF7b/2+fUqtYxcuP5bkxo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zL2f6Rc-JG54","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1663123056572,"user_tz":-540,"elapsed":131959,"user":{"displayName":"이상용","userId":"06393511036456385624"}},"outputId":"0f226c84-170f-4e65-f748-1c5a3d360b54"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","import os\n","import sys\n","from datetime import datetime\n","\n","drive_project_root = '/content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus'\n","sys.path.append(drive_project_root)\n","!pip install -r '/content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt'"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-lightning==1.3.8\n","  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n","\u001b[K     |████████████████████████████████| 813 kB 7.9 MB/s \n","\u001b[?25hCollecting torch-optimizer==0.1.0\n","  Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n","\u001b[K     |████████████████████████████████| 72 kB 1.1 MB/s \n","\u001b[?25hCollecting hydra-core==1.1\n","  Downloading hydra_core-1.1.0-py3-none-any.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 73.9 MB/s \n","\u001b[?25hCollecting wandb==0.11.1\n","  Downloading wandb-0.11.1-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 41.3 MB/s \n","\u001b[?25hCollecting torchtext==0.10.0\n","  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 11.2 MB/s \n","\u001b[?25hCollecting spacy==2.2.4\n","  Downloading spacy-2.2.4-cp37-cp37m-manylinux1_x86_64.whl (10.6 MB)\n","\u001b[K     |████████████████████████████████| 10.6 MB 24.2 MB/s \n","\u001b[?25hCollecting efficientnet_pytorch==0.7.1\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","Collecting tensorflow-addons==0.14.0\n","  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 25.0 MB/s \n","\u001b[?25hCollecting torchmetrics>=0.2.0\n","  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n","\u001b[K     |████████████████████████████████| 419 kB 66.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (4.64.1)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.12.1+cu113)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.21.6)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (2022.8.2)\n","Requirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (7.1.2)\n","Collecting pyDeprecate==0.3.0\n","  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n","Collecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 67.1 MB/s \n","\u001b[?25hCollecting PyYAML<=5.4.1,>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 51.1 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (2.8.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (21.3)\n","Collecting pytorch-ranger>=0.1.1\n","  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n","Collecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 64.7 MB/s \n","\u001b[?25hCollecting omegaconf==2.1.*\n","  Downloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core==1.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 3)) (5.9.0)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (2.23.0)\n","Collecting graphql-core>=2.3.0\n","  Downloading graphql_core-3.2.1-py3-none-any.whl (202 kB)\n","\u001b[K     |████████████████████████████████| 202 kB 75.1 MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 67.5 MB/s \n","\u001b[?25hCollecting configparser>=3.8.1\n","  Downloading configparser-5.3.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (3.17.3)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 70.0 MB/s \n","\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (2.3)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (5.4.8)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (7.1.2)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (2.8.2)\n","Collecting torch>=1.4\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.7 kB/s \n","\u001b[?25hCollecting thinc==7.4.0\n","  Downloading thinc-7.4.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 53.2 MB/s \n","\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n","  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n","Collecting catalogue<1.1.0,>=0.0.7\n","  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 6)) (2.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 6)) (3.0.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 6)) (57.4.0)\n","Collecting srsly<1.1.0,>=1.0.2\n","  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 74.6 MB/s \n","\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 6)) (0.10.1)\n","Collecting blis<0.5.0,>=0.4.0\n","  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n","\u001b[K     |████████████████████████████████| 3.7 MB 69.7 MB/s \n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 6)) (1.0.8)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.14.0->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 8)) (2.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (4.1.1)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 6)) (4.12.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (3.8.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (2.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (22.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (0.13.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.2.0)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 6)) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb==0.11.1->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 4)) (2022.6.15)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 63.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 74.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 75.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 65.2 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 77.1 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 72.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 67.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 67.4 MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (0.6.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (0.37.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (0.4.6)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.2.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.48.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (3.4.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning==1.3.8->-r /content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/requirements.txt (line 1)) (3.2.0)\n","Building wheels for collected packages: efficientnet-pytorch, antlr4-python3-runtime, future, pathtools\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=d3e5eedd6605e2917909bad97bf777d65a61439ab804aff3fab67cee3f2741a2\n","  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=a4fd85794b331d7b257128fd6b3557289fb4fa3985a4215a3e02053b9cf76233\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=02771023e4b56d32475d55974291b2cc4eb1c6912d18865f508d4c3822f625cd\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=e3ad90d0cb99da40fc24840701f68c8e982519d3097247d22a9cfa1ff26ad91c\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built efficientnet-pytorch antlr4-python3-runtime future pathtools\n","Installing collected packages: smmap, torch, srsly, PyYAML, plac, gitdb, catalogue, blis, antlr4-python3-runtime, torchmetrics, thinc, shortuuid, sentry-sdk, pytorch-ranger, pyDeprecate, pathtools, omegaconf, graphql-core, GitPython, future, docker-pycreds, configparser, wandb, torchtext, torch-optimizer, tensorflow-addons, spacy, pytorch-lightning, hydra-core, efficientnet-pytorch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 2.4.4\n","    Uninstalling srsly-2.4.4:\n","      Successfully uninstalled srsly-2.4.4\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 2.0.8\n","    Uninstalling catalogue-2.0.8:\n","      Successfully uninstalled catalogue-2.0.8\n","  Attempting uninstall: blis\n","    Found existing installation: blis 0.7.8\n","    Uninstalling blis-0.7.8:\n","      Successfully uninstalled blis-0.7.8\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 8.1.0\n","    Uninstalling thinc-8.1.0:\n","      Successfully uninstalled thinc-8.1.0\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.13.1\n","    Uninstalling torchtext-0.13.1:\n","      Successfully uninstalled torchtext-0.13.1\n","  Attempting uninstall: spacy\n","    Found existing installation: spacy 3.4.1\n","    Uninstalling spacy-3.4.1:\n","      Successfully uninstalled spacy-3.4.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n","en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.4 which is incompatible.\u001b[0m\n","Successfully installed GitPython-3.1.27 PyYAML-5.4.1 antlr4-python3-runtime-4.8 blis-0.4.1 catalogue-1.0.0 configparser-5.3.0 docker-pycreds-0.4.0 efficientnet-pytorch-0.7.1 future-0.18.2 gitdb-4.0.9 graphql-core-3.2.1 hydra-core-1.1.0 omegaconf-2.1.2 pathtools-0.1.2 plac-1.1.3 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 pytorch-ranger-0.1.1 sentry-sdk-1.9.0 shortuuid-1.0.9 smmap-5.0.0 spacy-2.2.4 srsly-1.0.5 tensorflow-addons-0.14.0 thinc-7.4.0 torch-1.9.0 torch-optimizer-0.1.0 torchmetrics-0.9.3 torchtext-0.10.0 wandb-0.11.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"IIO5LG2wl70Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663123213096,"user_tz":-540,"elapsed":386,"user":{"displayName":"이상용","userId":"06393511036456385624"}},"outputId":"d2acc83a-61af-49cb-a53a-4dfb363cc2f5"},"source":["gpu_info = !nvidia-smi\n","gpu_info = \"\\n\".join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","    print('Select the Runtime > \"change runtime type\" menu to enable a GPU accelerator, ')\n","    print('and then re-execurte this cell.')\n","else:\n","    print(gpu_info)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Sep 14 02:40:12 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"35FDJKXxJ_to","executionInfo":{"status":"error","timestamp":1663123220700,"user_tz":-540,"elapsed":1382,"user":{"displayName":"이상용","userId":"06393511036456385624"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b6576061-b60a-4d50-dc0c-b1de2fa07797"},"source":["# For data loading\n","from typing import List\n","from typing import Dict\n","from typing import Union\n","from typing import Any\n","from typing import Optional\n","from typing import Iterable\n","from typing import Callable\n","from abc import abstractmethod\n","from abc import ABC\n","from datetime import datetime\n","from functools import partial\n","from collections import Counter\n","from collections import OrderedDict\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import torch\n","import math\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.nn import Transformer\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","from pprint import pprint\n","\n","from torchtext import data\n","from torchtext import datasets\n","from torchtext.datasets import Multi30k\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.data.functional import to_map_style_dataset\n","from torchtext.vocab import Vocab, build_vocab_from_iterator, vocab\n","import spacy\n","\n","# For configuration\n","from omegaconf import DictConfig\n","from omegaconf import OmegaConf\n","import hydra\n","from hydra.core.config_store import ConfigStore\n","\n","# For logger\n","from torch.utils.tensorboard import SummaryWriter\n","import wandb\n","os.environ[\"WANDB_START_METHOD\"]=\"thread\""],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n","  warn(f\"Failed to load image Python extension: {e}\")\n"]},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-420b030dda5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0m_PROJECT_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_PACKAGE_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningDataModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLightningModule\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from pytorch_lightning.metrics.classification import (  # noqa: F401\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mAccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mAUC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/classification/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccuracy\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUC\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauroc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUROC\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/classification/accuracy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccuracy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdim_zero_mean\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_dim_zero_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdim_zero_sum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_dim_zero_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_num_classes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_num_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselect_topk\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_select_topk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_to_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'get_num_classes' from 'torchmetrics.utilities.data' (/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/data.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"qw2Pp_hxO-Iw","executionInfo":{"status":"error","timestamp":1663123375459,"user_tz":-540,"elapsed":1025,"user":{"displayName":"이상용","userId":"06393511036456385624"}},"colab":{"base_uri":"https://localhost:8080/","height":572},"outputId":"b562ac56-e17a-4063-9ec1-812a7cd4e8e1"},"source":["from data_utils import dataset_split\n","from config_utils import flatten_dict\n","from config_utils import register_config\n","from config_utils import configure_optimizers_from_cfg\n","from config_utils import get_loggers\n","from config_utils import get_callbacks"],"execution_count":6,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-758ef9e5f34f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflatten_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigure_optimizers_from_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/[강의자료] 한 번에 끝내는 딥러닝-인공지능 초격차 패키지 (수정, 배포 금지)_/Part 5. 딥러닝 기초 알고리즘 및 최신 트렌드 알고리즘_/강의자료/#fastcampus/config_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_optimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_optimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhydra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0m_PROJECT_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_PACKAGE_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningDataModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLightningModule\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m from pytorch_lightning.metrics.classification import (  # noqa: F401\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mAccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mAUC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/classification/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccuracy\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUC\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauroc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUROC\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/classification/accuracy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccuracy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/metrics/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdim_zero_mean\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_dim_zero_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdim_zero_sum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_dim_zero_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_num_classes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_num_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselect_topk\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_select_topk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_to_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'get_num_classes' from 'torchmetrics.utilities.data' (/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/data.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"UQCJwE0bKa2_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640334749087,"user_tz":-540,"elapsed":31415,"user":{"displayName":"이상용","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06393511036456385624"}},"outputId":"2db9f9eb-9cef-4c95-ddeb-0adf01f0572f"},"source":["# download spacy data.\n","!python -m spacy download en\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de\n","!python -m spacy download de_core_news_sm\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 9.9 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n","Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 11.4 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","Collecting de_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n","\u001b[K     |████████████████████████████████| 14.9 MB 13.5 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n","Building wheels for collected packages: de-core-news-sm\n","  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-py3-none-any.whl size=14907055 sha256=3edca3d6af3bf0bebce0b235d292f33d60e88b4b45d9292ccaf58c2c29d6e435\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-iurfj8tq/wheels/00/66/69/cb6c921610087d2cab339062345098e30a5ceb665360e7b32a\n","Successfully built de-core-news-sm\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('de_core_news_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/de\n","You can now load the model via spacy.load('de')\n","Collecting de_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n","\u001b[K     |████████████████████████████████| 14.9 MB 13.3 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('de_core_news_sm')\n"]}]},{"cell_type":"code","metadata":{"id":"p3tQ9h4ZdLim"},"source":["data_spacy_de_en_cfg = {\n","    \"name\": \"spacy_de_en\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n","    'tokenizer': \"spacy\",\n","    \"src_lang\": \"de\",\n","    \"tgt_lang\": \"en\",\n","    \"src_index\": 0,\n","    \"tgt_index\": 1,\n","    \"vocab\": {\n","        \"special_symbol2index\": {\n","            \"<unk>\": 0,\n","            \"<pad>\": 1,\n","            \"<bos>\": 2,\n","            \"<eos>\": 3,\n","            \n","        },\n","        \"special_first\": True,\n","        \"min_freq\": 2\n","    }\n","}\n","\n","data_cfg = OmegaConf.create(data_spacy_de_en_cfg)\n","# print(OmegaConf.to_yaml(data_cfg))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_Qeg417e7Pm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640334764339,"user_tz":-540,"elapsed":1233,"user":{"displayName":"이상용","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06393511036456385624"}},"outputId":"79216c1e-ca50-478e-d54f-dcd22149a530"},"source":["train_data, vaild_data, test_data = Multi30k(data_cfg.data_root)\n","test_data = to_map_style_dataset(test_data)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 5.28MB/s]\n","validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 1.84MB/s]\n","mmt16_task1_test.tar.gz: 100%|██████████| 43.9k/43.9k [00:00<00:00, 1.71MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"bGcXL8ecfSKw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640334778818,"user_tz":-540,"elapsed":377,"user":{"displayName":"이상용","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06393511036456385624"}},"outputId":"6aa3a0de-ef78-4132-f15a-d7d974277a5f"},"source":["for i in test_data:\n","    print(i)\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\\n', 'A man in an orange hat starring at something.\\n')\n"]}]},{"cell_type":"code","metadata":{"id":"9TSP_X0Rf9VW"},"source":["# 1. token_transform (token ...)\n","\n","def get_token_transform(data_cfg: DictConfig) -> dict:\n","    token_transform: dict = {}\n","    token_transform[data_cfg.src_lang] = get_tokenizer(\n","        data_cfg.tokenizer, language=data_cfg.src_lang\n","    )\n","    token_transform[data_cfg.tgt_lang] = get_tokenizer(\n","        data_cfg.tokenizer, language=data_cfg.tgt_lang\n","    )\n","    return token_transform\n","\n","token_transform = get_token_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvXdvgOkgCKE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640334990068,"user_tz":-540,"elapsed":8218,"user":{"displayName":"이상용","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06393511036456385624"}},"outputId":"89ab703a-6548-440b-eef3-5c31d3958562"},"source":["# 2. vocab_transform\n","def yield_tokens(\n","    data_iter: Iterable, lang: str, lang2index: Dict[str, int],\n",") -> List[str]:\n","    \"\"\"help function to yield list of tokens\"\"\"\n","    for data_sample in data_iter:\n","        yield token_transform[lang](data_sample[lang2index[lang]])\n","\n","\n","def get_vocab_transform(data_cfg: DictConfig) -> dict:\n","    vocab_transform: dict = {}\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        train_iter = Multi30k(\n","            split=\"train\", language_pair=(data_cfg.src_lang, data_cfg.tgt_lang)\n","\n","        )\n","        \n","        # create torchtext's Vocab object\n","        vocab_transform[ln] = build_vocab_from_iterator(\n","            yield_tokens(\n","                train_iter,\n","                ln,\n","                {\n","                    data_cfg.src_lang: data_cfg.src_index,\n","                    data_cfg.tgt_lang: data_cfg.tgt_index\n","                }\n","            ),\n","            min_freq=data_cfg.vocab.min_freq,\n","            specials=list(data_cfg.vocab.special_symbol2index.keys()),\n","            special_first=True\n","\n","        )\n","\n","    # set UNKNOWN as the default index. --> index가 unknown으로 return  : token이 찾아지지 않은 경우 !\n","    # 만양에 세팅되지 않으면, runtime error가 날 수 있다!\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        vocab_transform[ln].set_default_index(data_cfg.vocab.special_symbol2index[\"<unk>\"])\n","    return vocab_transform\n"," \n","vocab_transform = get_vocab_transform(data_cfg)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 5.53MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"15XpkacvC0ld","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640335001226,"user_tz":-540,"elapsed":525,"user":{"displayName":"이상용","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06393511036456385624"}},"outputId":"aaa3d4bc-bc34-46c0-e2eb-4254faf3a528"},"source":["print(vocab_transform[\"de\"][\"<bos>\"])\n","print(vocab_transform[\"en\"][\"hello\"], vocab_transform[\"en\"][\"world\"])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","5466 1871\n"]}]},{"cell_type":"code","metadata":{"id":"d8b7HMM1gF4t"},"source":["# 3 integrated transforms\n","# --> text_transform: [token_transform --> vocab_transform -->torch.tensor transform]\n","\n","# helper function for collate_fn\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# convert to torch.tensor with bos & eos\n","def tensor_transform(token_ids: List[int], bos_index: int, eos_index: int):\n","    return torch.cat(\n","        (torch.tensor([bos_index]), torch.tensor(token_ids), torch.tensor([eos_index]))\n","    )\n","\n","# src & tgt lang language text_transforms to convert raw strings --> tensor indices\n","def get_text_transform(data_cfg: DictConfig):\n","    text_transform = {}\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        text_transform[ln] = sequential_transforms(\n","            token_transform[ln],\n","            vocab_transform[ln],\n","            partial(\n","                tensor_transform,\n","                bos_index=data_cfg.vocab.special_symbol2index[\"<bos>\"],\n","                eos_index=data_cfg.vocab.special_symbol2index[\"<eos>\"]\n","            )\n","        )\n","    return text_transform\n","\n","text_transform = get_text_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rnuSw7WlCr5a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640335134081,"user_tz":-540,"elapsed":387,"user":{"displayName":"이상용","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06393511036456385624"}},"outputId":"c246d9c2-27b9-4937-d12a-789571718f33"},"source":["print(text_transform['en'](\"hello\"))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([   2, 5466,    3])\n"]}]},{"cell_type":"code","metadata":{"id":"SwOtRipjgxst","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640335199045,"user_tz":-540,"elapsed":400,"user":{"displayName":"이상용","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06393511036456385624"}},"outputId":"0723bea3-e40d-44f8-fda2-f69f40904e11"},"source":["# 4 collate_fn -->batch를 전처리 할까?\n","def collate_fn(batch, data_cfg: DictConfig):\n","    src_batch, tgt_batch = [], []\n","\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[data_cfg.src_lang](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[data_cfg.tgt_lang](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=data_cfg.vocab.special_symbol2index['<pad>'])\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=data_cfg.vocab.special_symbol2index['<pad>'])\n","    return src_batch, tgt_batch\n","\n","def get_collate_fn(cfg: DictConfig):\n","    return partial(collate_fn, data_cfg=cfg.data)\n","\n","# 5 data loader\n","def get_multi30k_dataloader(split_mode: str, language_pair: tuple, batch_size: int, collate_fn):\n","\n","    iter = Multi30k(split=split_mode, language_pair=language_pair)\n","    dataset = to_map_style_dataset(iter)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n","    return dataloader\n","\n","test_dataloader = get_multi30k_dataloader(\"test\", (data_cfg.src_lang, data_cfg.tgt_lang), 3, collate_fn=partial(collate_fn, data_cfg=data_cfg))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["mmt16_task1_test.tar.gz: 100%|██████████| 43.9k/43.9k [00:00<00:00, 1.83MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"CogVCNgW19dh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640335202627,"user_tz":-540,"elapsed":402,"user":{"displayName":"이상용","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06393511036456385624"}},"outputId":"587c0960-173b-46db-993f-93012cddb508"},"source":["for i in test_dataloader:\n","    print(i)\n","    break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([[   2,    2,    2],\n","        [   6,    6,    6],\n","        [  13, 3690,   28],\n","        [  11, 4018,    8],\n","        [   7,   87,    7],\n","        [ 179,   44, 2874],\n","        [ 109,    0, 3030],\n","        [   9,  121,   21],\n","        [  17,   29,  296],\n","        [  79,    7,   11],\n","        [   0,   53,    7],\n","        [   5,  328, 4946],\n","        [   3,    5,    5],\n","        [   1,    3,    3]]), tensor([[   2,    2,    2],\n","        [   7,    7,    7],\n","        [  13, 3375,   34],\n","        [   8, 4933,    8],\n","        [  29,   11,  873],\n","        [  92,   83,  235],\n","        [  69,   10, 3454],\n","        [2671, 2603,    4],\n","        [  21,   52,  344],\n","        [ 123,  102,   15],\n","        [   6,    8,    4],\n","        [   3,   45,   45],\n","        [   1,   14,  841],\n","        [   1,    4,    6],\n","        [   1,   25,    3],\n","        [   1,  275,    1],\n","        [   1,    6,    1],\n","        [   1,    3,    1]]))\n"]}]},{"cell_type":"code","metadata":{"id":"_1AObd1SCabC"},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zp6kXTyCJY2N"},"source":["def _text_postprocessing(res: List[str]) -> str:\n","    if \"<eos>\" in res:\n","        res = res[:res.index(\"<eos>\")]\n","    if \"<pad>\" in res:\n","        res = res[:res.index(\"<pad>\")]\n","    res = \" \".join(res).replace(\"<bos>\", \"\")\n","    return res\n","\n","\n","\n","class BaseTranslatedLightningModule(pl.LightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.loss_function = torch.nn.CrossEntropyLoss(\n","            ignore_index=cfg.data.vocab.special_symbol2index[\"<pad>\"]\n","        )\n","\n","    def configure_optimizers(self):\n","        self._optimizers, self._scheduler = configure_optimizers_from_cfg(\n","            self.cfg, self\n","        )\n","        return self._optimizers, self._scheduler\n","\n","    @abstractmethod\n","    def forward(self, src, tgt, teacher_forcing_ratio: float):\n","        raise NotImplementedError()\n","\n","    def _forward(self, src, tgt, mode: str, teacher_forcing_ratio: float = 0.5):\n","        # teacher forcing:\n","        # seq2seq 에서 많이 쓰인다\n","        # src -> tgt autoregressive 학습하면, 맨 최소는 학습을 빠르게 하지만, 미래부분 학습은 기다리기 너무 힘들다\n","        # 랜덤으로 미래 정보도 조금 둬서 뒤에 있는 정보도 학습이 가능하게함\n","        # 0.5 확룰로 teacher_forcing을 하겠다\n","\n","        assert mode in [\"train\", \"val\", \"test\"]\n","\n","        # get predictions\n","        # teacher_forcing 용 input -->\n","        tgt_inputs = tgt[:-1, :] # delete ends for teacher forcing inputs\n","        outputs = self(src, tgt_inputs, teacher_forcing_ratio=teacher_forcing_ratio)\n","        tgt_outputs = tgt[1:, :] # delete start tokens\n","\n","        loss = self.loss_function(\n","            outputs.reshape(-1, outputs.shape[-1]), # [[batch X Seq_size], other_output_shape]\n","            tgt_outputs.reshape(-1),\n","        )\n","\n","        logs_detail = {\n","            f\"{mode}_src\": src,\n","            f\"{mode}_tgt\": tgt,\n","            f\"{mode}_results\": outputs,\n","        }\n","\n","        if mode in [\"val\", \"test\"]:\n","            _, tgt_results = torch.max(outputs, dim=2)\n","\n","            src_texts = []\n","            tgt_texts = []\n","            res_texts = []\n","\n","            # convert [L X B X others] --> [b X L X others]\n","            for src_i in torch.transpose(src, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.src_lang].lookup_tokens(src_i)\n","                src_texts.append(_text_postprocessing(res))\n","\n","            for tgt_i in torch.transpose(tgt, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_i)\n","                tgt_texts.append(_text_postprocessing(res))\n","\n","            for tgt_res_i in torch.transpose(tgt_results, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n","                res_texts.append(_text_postprocessing(res))\n","\n","            text_result_summary = {\n","                f\"{mode}_src_text\": src_texts,\n","                f\"{mode}_tgt_text\": tgt_texts,\n","                f\"{mode}_results_text\": res_texts,\n","            }\n","            print(f\"{self.global_step} step: \\n src_text: {src_texts[0]}, \\n tgt_text: {tgt_texts[0]}, \\n result_text: {res_texts[0]}\")\n","            logs_detail.update(text_result_summary)\n","\n","        return {f\"{mode}_loss\":loss}, logs_detail\n","\n","    def training_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, _ = self._forward(src, tgt, \"train\", self.cfg.model.teacher_forcing_ratio)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"train_loss\"]\n","        return logs\n","\n","    def validation_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"val\", 0.0)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"val_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","\n","    def test_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"test\", 0.0)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"test_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","\n","class TransformerTranslatedLightningModule(BaseTranslatedLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","\n","    def configure_optimizers(self):\n","        self._optimizers, self._scheduler = configure_optimizers_from_cfg(\n","            self.cfg, self\n","        )\n","        return self._optimizers, self._scheduler\n","\n","    @abstractmethod\n","    def forward(self, src, tgt):\n","        raise NotImplementedError()\n","\n","    def _forward(self, src, tgt, mode: str):\n","        # teacher forcing:\n","        # seq2seq 에서 많이 쓰인다\n","        # src -> tgt autoregressive 학습하면, 맨 최소는 학습을 빠르게 하지만, 미래부분 학습은 기다리기 너무 힘들다\n","        # 랜덤으로 미래 정보도 조금 둬서 뒤에 있는 정보도 학습이 가능하게함\n","        # 0.5 확룰로 teacher_forcing을 하겠다\n","\n","        assert mode in [\"train\", \"val\", \"test\"]\n","\n","        # get predictions\n","        # teacher_forcing 용 input -->\n","        tgt_inputs = tgt[:-1, :] # delete ends\n","        outputs = self(src, tgt_inputs)\n","        tgt_outputs = tgt[1:, :] # delete start tokens\n","\n","        loss = self.loss_function(\n","            outputs.reshape(-1, outputs.shape[-1]), # [[batch X Seq_size], other_output_shape]\n","            tgt_outputs.reshape(-1),\n","        )\n","\n","        logs_detail = {\n","            f\"{mode}_src\": src,\n","            f\"{mode}_tgt\": tgt,\n","            f\"{mode}_results\": outputs,\n","        }\n","\n","        if mode in [\"val\", \"test\"]:\n","            _, tgt_results = torch.max(outputs, dim=2)\n","\n","            src_texts = []\n","            tgt_texts = []\n","            res_texts = []\n","\n","            # convert [L X B X others] --> [b X L X others]\n","            for src_i in torch.transpose(src, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.src_lang].lookup_tokens(src_i)\n","                src_texts.append(_text_postprocessing(res))\n","\n","            for tgt_i in torch.transpose(tgt, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_i)\n","                tgt_texts.append(_text_postprocessing(res))\n","\n","            for tgt_res_i in torch.transpose(tgt_results, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n","                res_texts.append(_text_postprocessing(res))\n","\n","            text_result_summary = {\n","                f\"{mode}_src_text\": src_texts,\n","                f\"{mode}_tgt_text\": tgt_texts,\n","                f\"{mode}_results_text\": res_texts,\n","            }\n","            print(f\"{self.global_step} step: \\n src_text: {src_texts[0]}, \\n tgt_text: {tgt_texts[0]}, \\n result_text: {res_texts[0]}\")\n","            logs_detail.update(text_result_summary)\n","\n","        return {f\"{mode}_loss\":loss}, logs_detail\n","\n","    def training_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, _ = self._forward(src, tgt, \"train\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"train_loss\"]\n","        return logs\n","\n","    def validation_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"val\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"val_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","\n","    def test_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"test\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"test_loss\"]\n","        logs.update(logs_detail)\n","        return logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_AT5VFyP6sq"},"source":["# untils for initializations\n","def init_weights(model: Union[nn.Module, pl.LightningModule]):\n","    for name, param in model.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzgYXgyhb2Ar"},"source":["# model definition\n","\n","# 1. encoder\n","class LSTMEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers =n_layers\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # initialization of weights\n","        self.apply(init_weights)\n","\n","    def forward(self, src):\n","        # src = [seq_len, batch_size]\n","        embedded = self.dropout(self.embedding(src)) # [seq_len, batch_size, emb_dim]\n","\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","\n","        # outputs = [src_len, batch_size, hidden_dim * n direction]\n","        # hidden, cell = [n layers * n directions, batch_size, hidden_dim]\n","\n","        # outputs will be used from top hidden layers\n","        return hidden, cell\n","\n","# 2. decoder\n","class LSTMDecoder(nn.Module):\n","    def __init__(\n","        self,\n","        output_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.n_layers =n_layers\n","\n","        self.output_dim = output_dim\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.fc_out = nn.Linear(hidden_dim , output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input, hidden, cell):\n","        # input: [batch size X ...] <- start token\n","\n","        # outputs = [src_len, batch_size, hidden_dim * n direction]\n","        # hidden, cell = [n layers * n directions, batch_size, hidden_dim]\n","\n","        input = input.unsqueeze(0) # <- [1, batch_size]\n","        embedded = self.dropout(self.embedding(input))\n","\n","        # embedding = [1, batch_size, embed_dim]\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","\n","        # output = [1, batch_size, hidden_size]\n","        # hidden, cell = [n layers * 1 directions, batch_size, hidden_dim]\n","\n","        prediction = self.fc_out(output.squeeze(0)) # [batch_size, output_size]\n","\n","        return prediction, hidden, cell\n","\n"," # 3. Seq2Seq(cfg) <-- encoder + decoder\n","class LSTMSeq2Seq(BaseTranslatedLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","\n","        self.encoder = LSTMEncoder(**cfg.model.enc)\n","        self.decoder = LSTMDecoder(**cfg.model.dec)\n","\n","        assert self.encoder.hidden_dim == self.decoder.hidden_dim\n","        assert self.encoder.n_layers == self.decoder.n_layers\n","         \n","        # paramerters 들 init.\n","        self.apply(init_weights)\n","\n","    def forward(self, src, tgt, teacher_forcing_ratio: float = 0.5):\n","\n","        # src, tgt = [seq_len(can be different), batch_size]\n","        # for val, test teacher forcing should be 0.0\n","\n","        batch_size = tgt.shape[1]\n","        tgt_len = tgt.shape[0]\n","        tgt_vocab_size = self.decoder.output_dim\n","\n","        # tensor to store decoder outputs\n","        outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n","\n","        hidden, cell = self.encoder(src)\n","\n","        # start_token_input (<sos> tokens)\n","        input = tgt[0, :]\n","\n","        for t in range(1, tgt_len):\n","            \n","            # get one cell's output\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            # set to all outpus results\n","            outputs[t] = output\n","\n","            # decide whether going to us teacher forcing or not.\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            top1 = output.argmax()\n","\n","            input = tgt[t] if teacher_force else top1\n","\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6dMxy6rzJza"},"source":["# Concat; Addictive Attention 기반의 모델 새로 정의\n","# encoder, decoder rnn이 다를 수 있다\n","\n","class BidirectionalGRUEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        embed_dim: int,\n","        enc_hidden_dim: int,\n","        dec_hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.n_layers =n_layers\n","\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.GRU(embed_dim, enc_hidden_dim, n_layers, bidirectional=True, dropout=dropout)\n","        self.fc = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # initialization of weights\n","        self.apply(init_weights)\n","\n","    def forward(self, src):\n","\n","        # src = [seq_len, batch_size]\n","        embedded = self.dropout(self.embedding(src)) # [seq_len, batch_size, emb_dim]\n","\n","        outputs, hidden = self.rnn(embedded)\n","        # outputs = [src_len, batch_size, hidden_dim * n direction]\n","        # hidden = [n layers * n directions, batch_size, hidden_dim]\n","\n","        # hidden -> [forward_1, backward_1, forward_2, backward_2, ...]\n","        # 우리가 필요한건 맨 마지막 layer의 forward backward 두개 concat 한개 필요\n","\n","        # encoder RNNs fed through a linear layer to connect decoder.\n","        hidden = torch.tanh(self.fc(\n","            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n","        ))\n","\n","        return outputs, hidden\n","\n","class ConcatAttention(nn.Module):\n","    def __init__(self, enc_hidden_dim: int, dec_hidden_dim: int):\n","        super().__init__()\n","        self.attn = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim, dec_hidden_dim)\n","        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n","\n","    def forward(self, hidden, encoder_outputs):\n","\n","        # hidden = [batch_size, dec_hidden_dim] => from decoder. (query)\n","        # encdoer_outputs = [src_len, batch_size, enc_hidden_dim * 2] (key, value)\n","\n","        batch_size = encoder_outputs.shape[1]\n","        src_len = encoder_outputs.shape[0]\n","\n","        # repeat decoder hidden state src_len time ...\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        # hidden: [batch_size, src_len, dec_hidden_dim]\n","        # encoder_outputs = [batch_size, src_len, enc_hidden_dim *2]\n","\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n","\n","        # energy = [batch_size, src_len, dec_hidden_dim]\n","\n","        attention = self.v(energy).squeeze(2)\n","\n","        # attention = [batch_size, src_len]\n","\n","        return F.softmax(attention, dim=1)\n","\n","class AttentionalRNNDecoder(nn.Module):\n","    def __init__(\n","        self,\n","        output_dim: int,\n","        embed_dim: int,\n","        enc_hidden_dim: int,\n","        dec_hidden_dim: int,\n","        n_layers: int,\n","        dropout: float,\n","        attention: nn.Module\n","    ):\n","        super().__init__()\n","        self.output_dim = output_dim\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","\n","        self.rnn = nn.GRU((enc_hidden_dim * 2) + embed_dim, dec_hidden_dim, n_layers, dropout=dropout)\n","\n","        self.fc_out = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim + embed_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, input, hidden, encoder_outputs):\n","\n","        # input [batch size] # start_token\n","        # hidden [batch_size, dec_hidden_dim]\n","        # encoder_outputs [src_len, batch_size, enc_hidden_dim*2]\n","\n","        input = input.unsqueeze(0) # input = [1, batch_size]\n","\n","        embedded = self.dropout(self.embedding(input)) # [1, batch_size, embed_dim]\n","\n","        a = self.attention(hidden, encoder_outputs) # [batch_size, src_len]\n","        a = a.unsqueeze(1) # [batch_size, 1, src_len]\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2) # [batch_size, src_len, enc_hidden_dim*2]\n","        weighted = torch.bmm(a, encoder_outputs) # [batch_size, 1, enc_hidden_dim*2]\n","        weighted = weighted.permute(1, 0, 2) # [1, batch_size, enc_hidden_dim*2]\n","\n","        rnn_input = torch.cat((embedded, weighted), dim=2) # [1, batch_size, (enc_hidden_dim*2 + embed_dim)]\n","\n","        # hidden unsqueeze : [1, batch_size, dec_hidden_dim]\n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0)) \n","        # output = [seq_len, batch_size, dec_hidden_dim * n driections] => [1, batch_size, dec_hidden_size]\n","        # hidden = [n layers * n_directions, batch_size, dec_hidden_dim] => [1, batch_size, dec_hidden_size]\n","\n","\n","        if not (output == hidden).all():\n","            raise AssertionError()\n","\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted = weighted.squeeze(0)\n","\n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1)) #[batch_size, ouput_dim]\n","\n","        return prediction, hidden.squeeze(0)\n","\n","class AttentionBaseSeq2Seq(BaseTranslatedLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","\n","        self.encoder = BidirectionalGRUEncoder(**cfg.model.enc)\n","        self.attention = ConcatAttention(**cfg.model.attention)\n","        self.decoder = AttentionalRNNDecoder(\n","            attention=self.attention, **cfg.model.dec\n","        )\n","\n","    def forward(self, src, tgt, teacher_forcing_ratio: float = 0.5):\n","\n","        # src, tgt = [seq_len(can be different), batch_size]\n","        # for val, test teacher forcing should be 0.0\n","\n","        batch_size = tgt.shape[1]\n","        tgt_len = tgt.shape[0]\n","        tgt_vocab_size = self.decoder.output_dim\n","\n","        # tensor to store decoder outputs\n","        outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # start_token_input (<sos> tokens)\n","        input = tgt[0, :]\n","\n","        for t in range(1, tgt_len):\n","            \n","            # get one cell's output\n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\n","\n","            # set to all outpus results\n","            outputs[t] = output\n","\n","            # decide whether going to us teacher forcing or not.\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            top1 = output.argmax()\n","\n","            input = tgt[t] if teacher_force else top1\n","\n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHwcWrCNV_YL"},"source":["# 1. tokenembedding ..\n","# 2. positional encoding..\n","# 3. nn.Transformer ..\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(\n","        self,\n","        embed_size: int,\n","        dropout: float,\n","        maxlen: int = 5000\n","    ):\n","        super().__init__()\n","        den = torch.exp(-torch.arange(0, embed_size, 2)*math.log(10000) / embed_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, embed_size))\n","\n","        # sin: 2i\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        # cos: 2i\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\"pos_embedding\", pos_embedding)\n","    \n","    def forward(self, token_embedding: torch.Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","\n","class TokenEmbedding(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        embed_size: int\n","    ):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.embed_size = embed_size\n","        \n","\n","    def forward(self, tokens: torch.Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.embed_size) # scaling for embed size\n","\n","\n","class TransformerSeq2Seq(TransformerTranslatedLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","        self.cfg = cfg\n","        num_encoder_layers = self.cfg.model.num_encoder_layers\n","        num_decoder_layers = self.cfg.model.num_decoder_layers\n","        embed_size = self.cfg.model.embed_size\n","        nhead = self.cfg.model.nhead\n","        src_vocab_size = self.cfg.model.src_vocab_size\n","        tgt_vocab_size = self.cfg.model.tgt_vocab_size\n","        dim_feedforward = self.cfg.model.dim_feedforward\n","        dropout = self.cfg.model.dropout\n","        \n","        self.transformer = Transformer(\n","            d_model = embed_size,\n","            nhead = nhead,\n","            num_encoder_layers = num_encoder_layers,\n","            num_decoder_layers = num_decoder_layers,\n","            dim_feedforward = dim_feedforward,\n","            dropout = dropout\n","        )\n","        self.generator = nn.Linear(embed_size, tgt_vocab_size)\n","        self.src_token_emb = TokenEmbedding(src_vocab_size, embed_size)\n","        self.tgt_token_emb = TokenEmbedding(tgt_vocab_size, embed_size)\n","        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n","\n","    def generate_square_subsequent_mask(self, sz: int):\n","        mask = (torch.triu(torch.ones((sz, sz), device=self.device)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def create_mask(self, src, tgt):\n","        src_seq_len = src.shape[0]\n","        tgt_seq_len = tgt.shape[0]\n","\n","        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n","        src_mask = torch.zeros((src_seq_len, src_seq_len), device=self.device).type(torch.bool)\n","\n","        src_padding_mask = (src == self.cfg.data.vocab.special_symbol2index[\"<pad>\"]).transpose(0, 1)\n","        tgt_padding_mask = (tgt == self.cfg.data.vocab.special_symbol2index[\"<pad>\"]).transpose(0, 1)\n","        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n","\n","    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n","        memory_key_padding_mask = src_padding_mask\n","\n","        src_emb = self.positional_encoding(self.src_token_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_token_emb(tgt))\n","        outs = self.transformer(\n","            src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask\n","        )\n","        return self.generator(outs)\n","\n","    def encoder(self, src: torch.Tensor, src_mask: torch.Tensor):\n","        return self.transformer.encoder(self.positional_encoding(self.src_token_emb(src)), src_mask)\n","\n","    def decoder(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n","        return self.transformer.decoder(self.positional_encoding(self.tgt_token_emb(tgt)), memory, tgt_mask)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSCGZBbVhOI8"},"source":["data_spacy_de_en_cfg = {\n","    \"name\": \"spacy_de_en\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n","    'tokenizer': \"spacy\",\n","    \"src_lang\": \"de\",\n","    \"tgt_lang\": \"en\",\n","    \"src_index\": 0,\n","    \"tgt_index\": 1,\n","    \"vocab\": {\n","        \"special_symbol2index\": {\n","            \"<unk>\": 0,\n","            \"<pad>\": 1,\n","            \"<bos>\": 2,\n","            \"<eos>\": 3,\n","            \n","        },\n","        \"special_first\": True,\n","        \"min_freq\": 2\n","    }\n","}\n","\n","data_cfg = OmegaConf.create(data_spacy_de_en_cfg)\n","\n","# get_dataset\n","train_data,vaild_data, test_data = Multi30k(data_cfg.data_root)\n","\n","token_transform = get_token_transform(data_cfg)\n","vocab_transform = get_vocab_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-BeLE3hiVzX"},"source":["# model configs\n","\n","model_translate_lstm_seq2seq_cfg = {\n","    \"name\": \"LSTMSeq2Seq\",\n","    'enc': {\n","        \"input_dim\": len(vocab_transform[data_cfg.src_lang]),\n","        \"embed_dim\": 256,\n","        \"hidden_dim\": 256,\n","        \"n_layers\": 2,\n","        \"dropout\": 0.5,\n","    },\n","    \"dec\": {\n","        \"output_dim\": len(vocab_transform[data_cfg.tgt_lang]),\n","        \"embed_dim\": 256,\n","        \"hidden_dim\": 256,\n","        \"n_layers\": 2,\n","        \"dropout\": 0.5,\n","    },\n","    \"teacher_forcing_ratio\": 0.5,\n","}\n","\n","model_translate_attention_based_seq2seq_cfg = {\n","    \"name\": \"AttentionBasedSeq2Seq\",\n","    'enc': {\n","        \"input_dim\": len(vocab_transform[data_cfg.src_lang]),\n","        \"embed_dim\": 256,\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","        \"n_layers\": 1,\n","        \"dropout\": 0.5,\n","    },\n","    \"dec\": {\n","        \"output_dim\": len(vocab_transform[data_cfg.tgt_lang]),\n","        \"embed_dim\": 256,\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","        \"n_layers\": 1,\n","        \"dropout\": 0.5,\n","    },\n","    \"attention\":{\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","    },\n","    \"teacher_forcing_ratio\": 0.5,\n","}\n","\n","model_translate_transformer_seq2seq_cfg = {\n","    \"name\": \"TransformerSeq2Seq\",\n","    \"num_encoder_layers\": 3,\n","    \"num_decoder_layers\": 3,\n","    \"embed_size\": 512,\n","    \"nhead\": 8,\n","    \"src_vocab_size\": len(vocab_transform[data_cfg.src_lang]),\n","    \"tgt_vocab_size\": len(vocab_transform[data_cfg.tgt_lang]),\n","    \"dim_feedforward\": 512,\n","    \"dropout\": 0.5,\n","}\n","\n","# opt_config\n","opt_cfg = {\n","    \"optimizers\": [\n","        {\n","            \"name\": \"RAdam\",\n","            \"kwargs\": {\n","                \"lr\": 1e-3,\n","            }\n","        }\n","    ],\n","    \"lr_schedulers\": [\n","        {\n","        \"name\": None,\n","        \"kwargs\": {\n","            \"warmup_end_steps\": 1000\n","        }\n","    }\n","    ]\n","}\n","\n","_merged_cfg_presets = {\n","    \"LSTM_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_lstm_seq2seq_cfg\n","    },\n","    \"attention_based_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_attention_based_seq2seq_cfg\n","    },\n","    \"transformer_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_transformer_seq2seq_cfg\n","    }\n","}\n","\n","# clear config hydra instance first\n","hydra.core.global_hydra.GlobalHydra.instance().clear()\n","\n","# register preset configs\n","register_config(_merged_cfg_presets)\n","\n","# initialization & compose configs\n","hydra.initialize(config_path=None)\n","cfg = hydra.compose('transformer_seq2seq_de_en_translate')\n","\n","# override some cfg\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n","\n","project_root_dir = os.path.join(\n","    drive_project_root, \"runs\", \"de_en_translate_tutorials\"\n",")\n","save_dir = os.path.join(project_root_dir, run_name)\n","run_root_dir = os.path.join(project_root_dir, run_name)\n","\n","# train configs\n","train_cfg = {\n","    \"train_batch_size\": 128,\n","    \"val_batch_size\": 32,\n","    \"test_batch_size\": 32,\n","    \"train_val_split\": [0.9, 0.1],\n","    \"run_root_dir\": run_root_dir,\n","    \"trainer_kwargs\": {\n","        \"accelerator\": \"dp\",\n","        \"gpus\": \"0\",\n","        \"max_epochs\": 50,\n","        \"val_check_interval\": 1.0,\n","        \"log_every_n_steps\": 100,\n","        \"flush_logs_every_n_steps\": 100,\n","    }\n","}\n","\n","# logger configs\n","log_cfg = {\n","    \"loggers\": {\n","        \"WandbLogger\": {\n","            \"project\": \"fastcampus_de_en_translate_tutorials\",\n","            'name': run_name,\n","            \"tags\": [\"fastcampus_de_en_translate_tutorials\"],\n","            \"save_dir\": run_root_dir,\n","        },\n","        \"TensorBoardLogger\": {\n","            \"save_dir\": project_root_dir,\n","            \"name\": run_name,\n","        }\n","    },\n","    \"callbacks\": {\n","        \"ModelCheckpoint\": {\n","            \"save_top_k\": 3,\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"verbose\": True,\n","            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n","            \"filename\": \"{epoch}-{val_loss:.3f}-{val_acc:.3f}\"\n","        },\n","        \"EarlyStopping\": {\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"patience\": 3,\n","            \"verbose\": True\n","        }\n","    }\n","}\n","\n","OmegaConf.set_struct(cfg, False)\n","cfg.train = train_cfg\n","cfg.log = log_cfg\n","# lock config\n","OmegaConf.set_struct(cfg, True)\n","print(OmegaConf.to_yaml(cfg))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1r501wRqkxVp"},"source":["# dataloader def\n","train_dataloader = get_multi30k_dataloader(\n","    \"train\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.train_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")\n","val_dataloader = get_multi30k_dataloader(\n","    \"valid\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.val_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")\n","test_dataloader = get_multi30k_dataloader(\n","    \"test\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.test_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4a1HSw9cnNSF"},"source":["# pl trainer def & get model\n","\n","def get_pl_model(cfg: DictConfig, checkpoint_path: Optional[str] = None):\n","\n","    if cfg.model.name == \"LSTMSeq2Seq\":\n","        model = LSTMSeq2Seq(cfg)\n","    elif cfg.model.name == \"AttentionBasedSeq2Seq\":\n","        model = AttentionBaseSeq2Seq(cfg)\n","    elif cfg.model.name == \"TransformerSeq2Seq\":\n","        model = TransformerSeq2Seq(cfg)\n","    else:\n","        raise NotImplementedError(\"Not implemented model\")\n","\n","    if checkpoint_path is not None:\n","        model = model.load_from_checkpoint(cfg, checkpoint_path=checkpoint_path)\n","    return model\n","\n","\n","model = None\n","model = get_pl_model(cfg)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPgBa1JpoqDo"},"source":["# pytorch lightning trainer def\n","logger = get_loggers(cfg)\n","callbacks = get_callbacks(cfg)\n","\n","trainer = pl.Trainer(\n","    callbacks=callbacks,\n","    logger=logger,\n","    default_root_dir = cfg.train.run_root_dir,\n","    num_sanity_val_steps=2,\n","    **cfg.train.trainer_kwargs\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1ZQ8Sdrsjm8"},"source":["trainer.fit(model, train_dataloader, val_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ByzddGoJvUEj"},"source":[],"execution_count":null,"outputs":[]}]}